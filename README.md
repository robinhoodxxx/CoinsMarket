
ğŸª™ CoinMarketCap Crypto Scraper

This project scrapes live cryptocurrency data from CoinMarketCap (https://coinmarketcap.com) using Selenium, BeautifulSoup, and parallel processing. It exports the results into a CSV file, with support for retrying failed pages and logging errors.

ğŸš€ Features

- âš¡ Parallel scraping with concurrent.futures
- ğŸ§­ Retries failed pages with isolated browser sessions
- ğŸ§  Thread-safe Singleton design for Chrome driver & logic classes
- ğŸ§¼ Clean architecture using Page Object Model (POM)
- ğŸ§¾ CSV export with auto-named timestamped files
- ğŸ—ƒ Logs failed pages to failed_pages.log

ğŸ“ Project Structure

CoinsMarket/
- â”œâ”€â”€ CsvFiles/                # Output CSV reports
- â”œâ”€â”€ failed_pages.log         # Logs failed pages during scrape
- â”œâ”€â”€ req.txt                  # Project dependencies
- â”œâ”€â”€ run.bat / run.sh         # Easy command-line runners
- â”œâ”€â”€ src/
- â”‚   â”œâ”€â”€ Runner/
- â”‚   â”‚   â””â”€â”€ parallel.py      # Entry point with parallel scraping
- â”‚   â”œâ”€â”€ hooks/
- â”‚   â”‚   â””â”€â”€ chromeDriver.py  # Chrome driver Singleton
- â”‚   â”œâ”€â”€ pages/
- â”‚   â”‚   â””â”€â”€ Home_AllCrypto_Page.py  # Page Object for CoinMarketCap
- â”‚   â”œâ”€â”€ steps/
- â”‚   â”‚   â””â”€â”€ ScrapeCoins_stepDef.py  # Parsing logic for coins
- â”‚   â””â”€â”€ utils/
- â”‚       â”œâ”€â”€ CsvImp.py        # CSV writer and page range input
- â”‚       â””â”€â”€ commonActions.py # Scrolling, waits, utility actions

ğŸ“¦ Setup

- git clone https://github.com/robinhoodxxx/CoinsMarket.git
- cd CoinsMarket
- pip install -r req.txt

Run on Windows we have two bat files:
 1. run.bat (series run like scrape page one after one)
 2. runParallel.bat (parallel execution based on chuck size 
   i.e no of pages you wanna scrape at a single time eg: chunk size is 5 then total pages is 50 you launched 50/5 ->10 browser parallel and scrape 5 pages in each browser)


ğŸ§ª How to Run man classes in cmd
python -m src.Runner.main (series execution)
This will:
1. Launch headless Chrome only once
2. Scrape the specified range of pages
4. Save the data to CsvFiles/<timestamp>.csv



python -m src.Runner.parallel (parallel execution)

This will:
1. Launch headless Chrome sessions in parallel
2. Scrape the specified range of pages
3. Retry failed pages individually
4. Save the data to CsvFiles/parallel_<timestamp>.csv
5. Log failed pages to failed_pages.log (as [1, 2, 5])

âš™ï¸ Configuration

Define the page range using a prompt or utility function in get_page_range(). CHUNK_SIZE controls how many pages are processed per thread.

ğŸ›  Dependencies

- selenium
- beautifulsoup4
- webdriver-manager

Install via:
- pip install -r req.txt

ğŸ“Œ Notes

- Ensure Google Chrome is installed (used by webdriver-manager)
- --disable-gpu improves headless stability but is optional locally
- Script automatically retries invalid session errors
- Compatible with Jenkins CI (CSV/logs can be archived as build artifacts)
